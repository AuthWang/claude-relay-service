#!/usr/bin/env node

/**
 * ğŸ—„ï¸ Claude Relay Service - æ•°æ®åº“åˆå§‹åŒ–è„šæœ¬
 *
 * è‡ªåŠ¨åˆ›å»ºPostgreSQLè¡¨ç»“æ„å’Œç´¢å¼•
 * Created by DevOps-Expert with SMART-6 optimization
 */

const { Pool } = require('pg')
const winston = require('winston')
const path = require('path')

// é…ç½®æ—¥å¿—
const logger = winston.createLogger({
  level: process.env.LOG_LEVEL || 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.colorize(),
    winston.format.printf(({ timestamp, level, message }) => `${timestamp} [${level}] ${message}`)
  ),
  transports: [
    new winston.transports.Console(),
    new winston.transports.File({
      filename: path.join(__dirname, '../logs/database-init.log'),
      maxsize: 10 * 1024 * 1024,
      maxFiles: 3
    })
  ]
})

class DatabaseInitializer {
  constructor() {
    this.pool = null
    this.tables = this.getTableDefinitions()
  }

  async initialize() {
    logger.info('ğŸ—„ï¸ åˆå§‹åŒ–æ•°æ®åº“è¿æ¥...')

    this.pool = new Pool({
      host: process.env.POSTGRES_HOST || 'localhost',
      port: process.env.POSTGRES_PORT || 5432,
      database: process.env.POSTGRES_DATABASE || 'claude_relay',
      user: process.env.POSTGRES_USER || 'postgres',
      password: String(process.env.POSTGRES_PASSWORD || ''),
      max: 5,
      idleTimeoutMillis: 30000,
      connectionTimeoutMillis: 5000,
      ssl:
        process.env.POSTGRES_SSL_ENABLED === 'true'
          ? {
              rejectUnauthorized: process.env.POSTGRES_SSL_REJECT_UNAUTHORIZED !== 'false'
            }
          : false
    })

    // æµ‹è¯•è¿æ¥
    const client = await this.pool.connect()
    await client.query('SELECT NOW()')
    client.release()

    logger.info('âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ')
  }

  getTableDefinitions() {
    return {
      // API Keysè¡¨
      api_keys: {
        schema: `
          CREATE TABLE IF NOT EXISTS api_keys (
            id VARCHAR(64) PRIMARY KEY,
            api_key_hash VARCHAR(128) UNIQUE NOT NULL,
            name VARCHAR(255) NOT NULL DEFAULT 'API Key',
            token_limit BIGINT NOT NULL DEFAULT 1000000,
            token_used BIGINT NOT NULL DEFAULT 0,
            is_active BOOLEAN NOT NULL DEFAULT true,
            expires_at TIMESTAMP,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
          )
        `,
        indexes: [
          'CREATE INDEX IF NOT EXISTS idx_api_keys_hash ON api_keys(api_key_hash)',
          'CREATE INDEX IF NOT EXISTS idx_api_keys_active ON api_keys(is_active) WHERE is_active = true',
          'CREATE INDEX IF NOT EXISTS idx_api_keys_expires ON api_keys(expires_at) WHERE expires_at IS NOT NULL'
        ],
        description: 'APIå¯†é’¥ç®¡ç†è¡¨'
      },

      // Claudeè´¦æˆ·è¡¨
      claude_accounts: {
        schema: `
          CREATE TABLE IF NOT EXISTS claude_accounts (
            id VARCHAR(64) PRIMARY KEY,
            name VARCHAR(255) NOT NULL,
            description TEXT DEFAULT '',
            claude_ai_oauth TEXT, -- åŠ å¯†å­˜å‚¨çš„OAuthæ•°æ®
            proxy_config JSONB,
            is_active BOOLEAN NOT NULL DEFAULT true,
            is_rate_limited BOOLEAN NOT NULL DEFAULT false,
            rate_limit_until TIMESTAMP,
            request_count BIGINT NOT NULL DEFAULT 0,
            last_used_at TIMESTAMP,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
          )
        `,
        indexes: [
          'CREATE INDEX IF NOT EXISTS idx_claude_accounts_active ON claude_accounts(is_active) WHERE is_active = true',
          'CREATE INDEX IF NOT EXISTS idx_claude_accounts_rate_limited ON claude_accounts(is_rate_limited, rate_limit_until) WHERE is_rate_limited = true',
          'CREATE INDEX IF NOT EXISTS idx_claude_accounts_last_used ON claude_accounts(last_used_at)',
          'CREATE INDEX IF NOT EXISTS idx_claude_accounts_proxy ON claude_accounts USING GIN(proxy_config) WHERE proxy_config IS NOT NULL'
        ],
        description: 'Claudeè´¦æˆ·ç®¡ç†è¡¨'
      },

      // Geminiè´¦æˆ·è¡¨
      gemini_accounts: {
        schema: `
          CREATE TABLE IF NOT EXISTS gemini_accounts (
            id VARCHAR(64) PRIMARY KEY,
            name VARCHAR(255) NOT NULL,
            description TEXT DEFAULT '',
            google_oauth TEXT, -- åŠ å¯†å­˜å‚¨çš„OAuthæ•°æ®
            proxy_config JSONB,
            is_active BOOLEAN NOT NULL DEFAULT true,
            is_rate_limited BOOLEAN NOT NULL DEFAULT false,
            rate_limit_until TIMESTAMP,
            request_count BIGINT NOT NULL DEFAULT 0,
            last_used_at TIMESTAMP,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
          )
        `,
        indexes: [
          'CREATE INDEX IF NOT EXISTS idx_gemini_accounts_active ON gemini_accounts(is_active) WHERE is_active = true',
          'CREATE INDEX IF NOT EXISTS idx_gemini_accounts_rate_limited ON gemini_accounts(is_rate_limited, rate_limit_until) WHERE is_rate_limited = true',
          'CREATE INDEX IF NOT EXISTS idx_gemini_accounts_last_used ON gemini_accounts(last_used_at)'
        ],
        description: 'Geminiè´¦æˆ·ç®¡ç†è¡¨'
      },

      // ç®¡ç†å‘˜è¡¨
      admins: {
        schema: `
          CREATE TABLE IF NOT EXISTS admins (
            id VARCHAR(64) PRIMARY KEY,
            username VARCHAR(255) UNIQUE NOT NULL,
            password_hash VARCHAR(255) NOT NULL,
            is_active BOOLEAN NOT NULL DEFAULT true,
            last_login_at TIMESTAMP,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
          )
        `,
        indexes: [
          'CREATE UNIQUE INDEX IF NOT EXISTS idx_admins_username ON admins(username)',
          'CREATE INDEX IF NOT EXISTS idx_admins_active ON admins(is_active) WHERE is_active = true'
        ],
        description: 'ç®¡ç†å‘˜è´¦æˆ·è¡¨'
      },

      // ä½¿ç”¨ç»Ÿè®¡è¡¨
      usage_statistics: {
        schema: `
          CREATE TABLE IF NOT EXISTS usage_statistics (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            date DATE NOT NULL,
            api_key_id VARCHAR(64) NOT NULL,
            model VARCHAR(100) NOT NULL,
            request_count BIGINT NOT NULL DEFAULT 0,
            token_count BIGINT NOT NULL DEFAULT 0,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            CONSTRAINT fk_usage_api_key FOREIGN KEY (api_key_id) REFERENCES api_keys(id) ON DELETE CASCADE
          )
        `,
        indexes: [
          'CREATE UNIQUE INDEX IF NOT EXISTS idx_usage_unique ON usage_statistics(date, api_key_id, model)',
          'CREATE INDEX IF NOT EXISTS idx_usage_date ON usage_statistics(date)',
          'CREATE INDEX IF NOT EXISTS idx_usage_api_key ON usage_statistics(api_key_id)',
          'CREATE INDEX IF NOT EXISTS idx_usage_model ON usage_statistics(model)',
          'CREATE INDEX IF NOT EXISTS idx_usage_date_model ON usage_statistics(date, model)'
        ],
        description: 'ä½¿ç”¨ç»Ÿè®¡è¡¨'
      },

      // ä¼šè¯è¡¨
      sessions: {
        schema: `
          CREATE TABLE IF NOT EXISTS sessions (
            id VARCHAR(128) PRIMARY KEY,
            admin_id VARCHAR(64) NOT NULL,
            data JSONB,
            expires_at TIMESTAMP NOT NULL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            CONSTRAINT fk_sessions_admin FOREIGN KEY (admin_id) REFERENCES admins(id) ON DELETE CASCADE
          )
        `,
        indexes: [
          'CREATE INDEX IF NOT EXISTS idx_sessions_admin ON sessions(admin_id)',
          'CREATE INDEX IF NOT EXISTS idx_sessions_expires ON sessions(expires_at)',
          'CREATE INDEX IF NOT EXISTS idx_sessions_data ON sessions USING GIN(data) WHERE data IS NOT NULL'
        ],
        description: 'ä¼šè¯ç®¡ç†è¡¨'
      },

      // ç³»ç»Ÿæ—¥å¿—è¡¨
      system_logs: {
        schema: `
          CREATE TABLE IF NOT EXISTS system_logs (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            level VARCHAR(20) NOT NULL,
            message TEXT NOT NULL,
            metadata JSONB,
            component VARCHAR(100),
            api_key_id VARCHAR(64),
            account_id VARCHAR(64),
            ip_address INET,
            user_agent TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
          )
        `,
        indexes: [
          'CREATE INDEX IF NOT EXISTS idx_logs_level ON system_logs(level)',
          'CREATE INDEX IF NOT EXISTS idx_logs_component ON system_logs(component)',
          'CREATE INDEX IF NOT EXISTS idx_logs_created_at ON system_logs(created_at)',
          'CREATE INDEX IF NOT EXISTS idx_logs_api_key ON system_logs(api_key_id) WHERE api_key_id IS NOT NULL',
          'CREATE INDEX IF NOT EXISTS idx_logs_account ON system_logs(account_id) WHERE account_id IS NOT NULL',
          'CREATE INDEX IF NOT EXISTS idx_logs_metadata ON system_logs USING GIN(metadata) WHERE metadata IS NOT NULL'
        ],
        description: 'ç³»ç»Ÿæ—¥å¿—è¡¨'
      },

      // ç³»ç»Ÿé…ç½®è¡¨
      system_configs: {
        schema: `
          CREATE TABLE IF NOT EXISTS system_configs (
            key VARCHAR(255) PRIMARY KEY,
            value JSONB NOT NULL,
            description TEXT,
            is_encrypted BOOLEAN NOT NULL DEFAULT false,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
          )
        `,
        indexes: [
          'CREATE INDEX IF NOT EXISTS idx_configs_encrypted ON system_configs(is_encrypted) WHERE is_encrypted = true'
        ],
        description: 'ç³»ç»Ÿé…ç½®è¡¨'
      }
    }
  }

  async createDatabase() {
    logger.info('ğŸ—ï¸ å¼€å§‹åˆ›å»ºæ•°æ®åº“è¡¨ç»“æ„...')

    const client = await this.pool.connect()

    try {
      await client.query('BEGIN')

      // ç¡®ä¿UUIDæ‰©å±•å­˜åœ¨
      await client.query('CREATE EXTENSION IF NOT EXISTS "uuid-ossp"')
      await client.query('CREATE EXTENSION IF NOT EXISTS "pg_stat_statements"')

      logger.info('âœ… æ•°æ®åº“æ‰©å±•å·²å¯ç”¨')

      // åˆ›å»ºè¡¨
      for (const [tableName, tableConfig] of Object.entries(this.tables)) {
        logger.info(`ğŸ“‹ åˆ›å»ºè¡¨: ${tableName}`)

        // åˆ›å»ºè¡¨ç»“æ„
        await client.query(tableConfig.schema)
        logger.info(`  âœ… è¡¨ç»“æ„åˆ›å»ºå®Œæˆ: ${tableName}`)

        // åˆ›å»ºç´¢å¼•
        for (const indexQuery of tableConfig.indexes) {
          await client.query(indexQuery)
        }
        logger.info(`  âœ… ç´¢å¼•åˆ›å»ºå®Œæˆ: ${tableName} (${tableConfig.indexes.length}ä¸ª)`)
      }

      // åˆ›å»ºè§¦å‘å™¨å‡½æ•°
      await this.createTriggers(client)

      // æ’å…¥åˆå§‹æ•°æ®
      await this.insertInitialData(client)

      await client.query('COMMIT')
      logger.info('ğŸ‰ æ•°æ®åº“ç»“æ„åˆ›å»ºå®Œæˆï¼')
    } catch (error) {
      await client.query('ROLLBACK')
      throw error
    } finally {
      client.release()
    }
  }

  async createTriggers(client) {
    logger.info('âš¡ åˆ›å»ºè§¦å‘å™¨...')

    // è‡ªåŠ¨æ›´æ–° updated_at å­—æ®µçš„è§¦å‘å™¨å‡½æ•°
    const updateTimestampFunction = `
      CREATE OR REPLACE FUNCTION update_updated_at_column()
      RETURNS TRIGGER AS $$
      BEGIN
        NEW.updated_at = CURRENT_TIMESTAMP;
        RETURN NEW;
      END;
      $$ language 'plpgsql';
    `

    await client.query(updateTimestampFunction)

    // ä¸ºéœ€è¦çš„è¡¨æ·»åŠ è§¦å‘å™¨
    const tablesWithUpdateTrigger = [
      'api_keys',
      'claude_accounts',
      'gemini_accounts',
      'admins',
      'usage_statistics',
      'sessions',
      'system_configs'
    ]

    for (const tableName of tablesWithUpdateTrigger) {
      const triggerQuery = `
        DROP TRIGGER IF EXISTS update_${tableName}_updated_at ON ${tableName};
        CREATE TRIGGER update_${tableName}_updated_at
          BEFORE UPDATE ON ${tableName}
          FOR EACH ROW
          EXECUTE FUNCTION update_updated_at_column();
      `

      await client.query(triggerQuery)
      logger.info(`  âœ… è§¦å‘å™¨åˆ›å»ºå®Œæˆ: ${tableName}`)
    }

    logger.info('âœ… æ‰€æœ‰è§¦å‘å™¨åˆ›å»ºå®Œæˆ')
  }

  async insertInitialData(client) {
    logger.info('ğŸ“ æ’å…¥åˆå§‹æ•°æ®...')

    // æ’å…¥ç³»ç»Ÿé…ç½®
    const systemConfigs = [
      {
        key: 'database_version',
        value: JSON.stringify({ version: '1.0.0', created_at: new Date().toISOString() }),
        description: 'æ•°æ®åº“ç‰ˆæœ¬ä¿¡æ¯'
      },
      {
        key: 'migration_status',
        value: JSON.stringify({ completed: false, last_migration: null }),
        description: 'æ•°æ®è¿ç§»çŠ¶æ€'
      },
      {
        key: 'system_stats',
        value: JSON.stringify({ initialized_at: new Date().toISOString() }),
        description: 'ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯'
      }
    ]

    for (const config of systemConfigs) {
      const insertQuery = `
        INSERT INTO system_configs (key, value, description)
        VALUES ($1, $2, $3)
        ON CONFLICT (key) DO UPDATE SET
        value = EXCLUDED.value,
        updated_at = CURRENT_TIMESTAMP
      `

      await client.query(insertQuery, [config.key, config.value, config.description])
    }

    logger.info('âœ… åˆå§‹æ•°æ®æ’å…¥å®Œæˆ')
  }

  async checkTableExists(tableName) {
    const client = await this.pool.connect()

    try {
      const result = await client.query(
        `
        SELECT EXISTS (
          SELECT FROM information_schema.tables
          WHERE table_schema = 'public'
          AND table_name = $1
        )
      `,
        [tableName]
      )

      return result.rows[0].exists
    } finally {
      client.release()
    }
  }

  async getTableInfo() {
    const client = await this.pool.connect()

    try {
      const result = await client.query(`
        SELECT
          t.table_name,
          t.table_type,
          c.column_count,
          i.index_count
        FROM information_schema.tables t
        LEFT JOIN (
          SELECT
            table_name,
            COUNT(*) as column_count
          FROM information_schema.columns
          WHERE table_schema = 'public'
          GROUP BY table_name
        ) c ON t.table_name = c.table_name
        LEFT JOIN (
          SELECT
            tablename,
            COUNT(*) as index_count
          FROM pg_indexes
          WHERE schemaname = 'public'
          GROUP BY tablename
        ) i ON t.table_name = i.tablename
        WHERE t.table_schema = 'public'
        AND t.table_type = 'BASE TABLE'
        ORDER BY t.table_name
      `)

      return result.rows
    } finally {
      client.release()
    }
  }

  async generateReport() {
    logger.info('ğŸ“Š ç”Ÿæˆæ•°æ®åº“æŠ¥å‘Š...')

    const tableInfo = await this.getTableInfo()
    const client = await this.pool.connect()

    try {
      const report = {
        summary: {
          totalTables: tableInfo.length,
          totalColumns: tableInfo.reduce((sum, table) => sum + (table.column_count || 0), 0),
          totalIndexes: tableInfo.reduce((sum, table) => sum + (table.index_count || 0), 0),
          createdAt: new Date().toISOString()
        },
        tables: {}
      }

      // è·å–æ¯ä¸ªè¡¨çš„è¯¦ç»†ä¿¡æ¯
      for (const table of tableInfo) {
        const tableName = table.table_name

        // è·å–è¡Œæ•°
        const countResult = await client.query(`SELECT COUNT(*) as count FROM ${tableName}`)
        const rowCount = parseInt(countResult.rows[0].count)

        report.tables[tableName] = {
          columns: table.column_count || 0,
          indexes: table.index_count || 0,
          rows: rowCount,
          description: this.tables[tableName]?.description || 'æœªçŸ¥è¡¨'
        }
      }

      // ä¿å­˜æŠ¥å‘Š
      const reportPath = path.join(__dirname, '../logs/database-report.json')
      require('fs').writeFileSync(reportPath, JSON.stringify(report, null, 2))

      logger.info('ğŸ“‹ æ•°æ®åº“åˆå§‹åŒ–æŠ¥å‘Š:')
      logger.info(`   è¡¨æ•°é‡: ${report.summary.totalTables}`)
      logger.info(`   å­—æ®µæ•°é‡: ${report.summary.totalColumns}`)
      logger.info(`   ç´¢å¼•æ•°é‡: ${report.summary.totalIndexes}`)
      logger.info(`   è¯¦ç»†æŠ¥å‘Š: ${reportPath}`)

      return report
    } finally {
      client.release()
    }
  }

  async cleanup() {
    if (this.pool) {
      await this.pool.end()
      logger.info('âœ… æ•°æ®åº“è¿æ¥å·²å…³é—­')
    }
  }
}

// CLIå…¥å£
async function main() {
  const initializer = new DatabaseInitializer()

  try {
    await initializer.initialize()

    // æ£€æŸ¥æ˜¯å¦å¼ºåˆ¶é‡æ–°åˆ›å»º
    const forceRecreate = process.argv.includes('--force')

    if (forceRecreate) {
      logger.warn('âš ï¸ å¼ºåˆ¶é‡æ–°åˆ›å»ºæ¨¡å¼ï¼Œå°†åˆ é™¤ç°æœ‰è¡¨')
      // è¿™é‡Œå¯ä»¥æ·»åŠ åˆ é™¤ç°æœ‰è¡¨çš„é€»è¾‘
    }

    // åˆ›å»ºæ•°æ®åº“ç»“æ„
    await initializer.createDatabase()

    // ç”ŸæˆæŠ¥å‘Š
    await initializer.generateReport()

    logger.info('ğŸ‰ æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸå®Œæˆï¼')
  } catch (error) {
    logger.error(`âŒ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: ${error.message}`)
    console.error('è¯¦ç»†é”™è¯¯:', error)
    process.exit(1)
  } finally {
    await initializer.cleanup()
  }
}

// å¦‚æœç›´æ¥è¿è¡Œæ­¤è„šæœ¬
if (require.main === module) {
  main().catch(console.error)
}

module.exports = DatabaseInitializer
