#!/usr/bin/env node

/**
 * ğŸ”„ Claude Relay Service - Redisåˆ°PostgreSQLæ•°æ®è¿ç§»å·¥å…·
 *
 * æ™ºèƒ½æ•°æ®è¿ç§»ç³»ç»Ÿï¼Œæ”¯æŒå¢é‡è¿ç§»å’Œæ•°æ®éªŒè¯
 * Created by DevOps-Expert with SMART-6 optimization
 */

const fs = require('fs')
const path = require('path')
const Redis = require('ioredis')
const { Pool } = require('pg')
const crypto = require('crypto')
const winston = require('winston')

// é…ç½®æ—¥å¿—
const logger = winston.createLogger({
  level: process.env.LOG_LEVEL || 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.colorize(),
    winston.format.printf(({ timestamp, level, message }) => `${timestamp} [${level}] ${message}`)
  ),
  transports: [
    new winston.transports.Console(),
    new winston.transports.File({
      filename: path.join(__dirname, '../logs/migration.log'),
      maxsize: 10 * 1024 * 1024,
      maxFiles: 5
    })
  ]
})

class DataMigrationService {
  constructor() {
    this.redisClient = null
    this.pgPool = null
    this.stats = {
      migrated: 0,
      failed: 0,
      skipped: 0,
      startTime: Date.now(),
      tables: {}
    }
    this.batchSize = parseInt(process.env.MIGRATION_BATCH_SIZE) || 100
    this.dryRun = process.env.MIGRATION_DRY_RUN === 'true'
  }

  async initialize() {
    logger.info('ğŸ”„ åˆå§‹åŒ–æ•°æ®è¿ç§»æœåŠ¡...')

    try {
      // åˆå§‹åŒ–Redisè¿æ¥
      this.redisClient = new Redis({
        host: process.env.REDIS_HOST || 'localhost',
        port: process.env.REDIS_PORT || 6379,
        password: process.env.REDIS_PASSWORD || undefined,
        db: process.env.REDIS_DB || 0
      })
      logger.info('âœ… Redisè¿æ¥å»ºç«‹æˆåŠŸ')

      // åˆå§‹åŒ–PostgreSQLè¿æ¥æ± 
      this.pgPool = new Pool({
        host: process.env.POSTGRES_HOST || 'localhost',
        port: process.env.POSTGRES_PORT || 5432,
        database: process.env.POSTGRES_DATABASE || 'claude_relay',
        user: process.env.POSTGRES_USER || 'postgres',
        password: process.env.POSTGRES_PASSWORD,
        max: 10,
        idleTimeoutMillis: 30000,
        connectionTimeoutMillis: 5000
      })

      // æµ‹è¯•PostgreSQLè¿æ¥
      const testClient = await this.pgPool.connect()
      await testClient.query('SELECT NOW()')
      testClient.release()
      logger.info('âœ… PostgreSQLè¿æ¥å»ºç«‹æˆåŠŸ')
    } catch (error) {
      logger.error(`âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: ${error.message}`)
      throw error
    }
  }

  async migrateData(options = {}) {
    const { tables = 'all', incremental = false, validate = true } = options

    logger.info(`ğŸš€ å¼€å§‹æ•°æ®è¿ç§» (æ¨¡å¼: ${this.dryRun ? 'DRY RUN' : 'PRODUCTION'})`)
    logger.info(`ğŸ“Š é…ç½®: tables=${tables}, incremental=${incremental}, validate=${validate}`)

    try {
      const migrationPlan = await this.createMigrationPlan(tables)
      logger.info(`ğŸ“‹ è¿ç§»è®¡åˆ’: ${migrationPlan.length}ä¸ªè¡¨`)

      for (const tablePlan of migrationPlan) {
        await this.migrateTable(tablePlan, incremental)

        if (validate) {
          await this.validateTableMigration(tablePlan)
        }
      }

      await this.generateMigrationReport()
      logger.info('ğŸ‰ æ•°æ®è¿ç§»å®Œæˆï¼')
    } catch (error) {
      logger.error(`âŒ è¿ç§»å¤±è´¥: ${error.message}`)
      await this.rollbackMigration()
      throw error
    }
  }

  async createMigrationPlan(tables) {
    const plan = []

    // API Keysè¿ç§»
    if (tables === 'all' || tables.includes('api_keys')) {
      plan.push({
        name: 'api_keys',
        redisPattern: 'api_key:*',
        pgTable: 'api_keys',
        transformer: this.transformApiKey.bind(this)
      })
    }

    // Claudeè´¦æˆ·è¿ç§»
    if (tables === 'all' || tables.includes('claude_accounts')) {
      plan.push({
        name: 'claude_accounts',
        redisPattern: 'claude_account:*',
        pgTable: 'claude_accounts',
        transformer: this.transformClaudeAccount.bind(this)
      })
    }

    // ç®¡ç†å‘˜è¿ç§»
    if (tables === 'all' || tables.includes('admins')) {
      plan.push({
        name: 'admins',
        redisPattern: 'admin:*',
        pgTable: 'admins',
        transformer: this.transformAdmin.bind(this)
      })
    }

    // ä½¿ç”¨ç»Ÿè®¡è¿ç§»
    if (tables === 'all' || tables.includes('usage_statistics')) {
      plan.push({
        name: 'usage_statistics',
        redisPattern: 'usage:*',
        pgTable: 'usage_statistics',
        transformer: this.transformUsageStats.bind(this)
      })
    }

    return plan
  }

  async migrateTable(tablePlan, incremental = false) {
    const { name, redisPattern, pgTable, transformer } = tablePlan

    logger.info(`ğŸ“‹ å¼€å§‹è¿ç§»è¡¨: ${name}`)
    this.stats.tables[name] = { migrated: 0, failed: 0, skipped: 0 }

    try {
      // è·å–Redisé”®åˆ—è¡¨
      const keys = await this.redisClient.keys(redisPattern)
      logger.info(`ğŸ” å‘ç° ${keys.length} æ¡è®°å½•åœ¨Redisä¸­`)

      // åˆ†æ‰¹å¤„ç†
      for (let i = 0; i < keys.length; i += this.batchSize) {
        const batch = keys.slice(i, i + this.batchSize)
        await this.processBatch(batch, transformer, pgTable, name, incremental)

        // è¿›åº¦æŠ¥å‘Š
        const progress = Math.round(((i + batch.length) / keys.length) * 100)
        logger.info(`ğŸ“Š ${name} è¿ç§»è¿›åº¦: ${progress}% (${i + batch.length}/${keys.length})`)
      }

      logger.info(`âœ… è¡¨ ${name} è¿ç§»å®Œæˆ`)
    } catch (error) {
      logger.error(`âŒ è¡¨ ${name} è¿ç§»å¤±è´¥: ${error.message}`)
      throw error
    }
  }

  async processBatch(keys, transformer, pgTable, tableName, incremental) {
    const client = await this.pgPool.connect()

    try {
      await client.query('BEGIN')

      for (const key of keys) {
        try {
          const redisData = await this.redisClient.get(key)
          if (!redisData) {
            this.stats.tables[tableName].skipped++
            continue
          }

          const parsedData = JSON.parse(redisData)
          const transformedData = await transformer(key, parsedData)

          if (!transformedData) {
            this.stats.tables[tableName].skipped++
            continue
          }

          // æ£€æŸ¥æ˜¯å¦å¢é‡è¿ç§»ä¸”è®°å½•å·²å­˜åœ¨
          if (incremental) {
            const existsQuery = `SELECT id FROM ${pgTable} WHERE id = $1`
            const existsResult = await client.query(existsQuery, [transformedData.id])

            if (existsResult.rows.length > 0) {
              this.stats.tables[tableName].skipped++
              continue
            }
          }

          if (!this.dryRun) {
            // æ„å»ºæ’å…¥æŸ¥è¯¢
            const columns = Object.keys(transformedData)
            const values = Object.values(transformedData)
            const placeholders = values.map((_, index) => `$${index + 1}`).join(', ')

            const insertQuery = `
              INSERT INTO ${pgTable} (${columns.join(', ')})
              VALUES (${placeholders})
              ON CONFLICT (id) DO UPDATE SET
              ${columns
                .slice(1)
                .map((col) => `${col} = EXCLUDED.${col}`)
                .join(', ')},
              updated_at = CURRENT_TIMESTAMP
            `

            await client.query(insertQuery, values)
          }

          this.stats.tables[tableName].migrated++
          this.stats.migrated++
        } catch (recordError) {
          logger.error(`âŒ è®°å½•è¿ç§»å¤±è´¥ ${key}: ${recordError.message}`)
          this.stats.tables[tableName].failed++
          this.stats.failed++
        }
      }

      await client.query('COMMIT')
    } catch (batchError) {
      await client.query('ROLLBACK')
      throw batchError
    } finally {
      client.release()
    }
  }

  // æ•°æ®è½¬æ¢å™¨
  async transformApiKey(key, data) {
    const keyId = key.replace('api_key:', '')

    return {
      id: keyId,
      api_key_hash: data.apiKeyHash || crypto.createHash('sha256').update(keyId).digest('hex'),
      name: data.name || 'Migrated Key',
      token_limit: parseInt(data.tokenLimit) || 1000000,
      token_used: parseInt(data.tokenUsed) || 0,
      is_active: data.isActive !== false,
      expires_at: data.expiresAt ? new Date(data.expiresAt) : null,
      created_at: data.createdAt ? new Date(data.createdAt) : new Date(),
      updated_at: new Date()
    }
  }

  async transformClaudeAccount(key, data) {
    const accountId = key.replace('claude_account:', '')

    return {
      id: accountId,
      name: data.name || 'Migrated Account',
      description: data.description || '',
      claude_ai_oauth: data.claudeAiOauth ? JSON.stringify(data.claudeAiOauth) : null,
      proxy_config: data.proxyConfig ? JSON.stringify(data.proxyConfig) : null,
      is_active: data.isActive !== false,
      is_rate_limited: data.isRateLimited === true,
      rate_limit_until: data.rateLimitUntil ? new Date(data.rateLimitUntil) : null,
      request_count: parseInt(data.requestCount) || 0,
      last_used_at: data.lastUsedAt ? new Date(data.lastUsedAt) : null,
      created_at: data.createdAt ? new Date(data.createdAt) : new Date(),
      updated_at: new Date()
    }
  }

  async transformAdmin(key, data) {
    const adminId = key.replace('admin:', '')

    return {
      id: adminId,
      username: data.username || 'admin',
      password_hash: data.passwordHash || data.password,
      is_active: data.isActive !== false,
      last_login_at: data.lastLoginAt ? new Date(data.lastLoginAt) : null,
      created_at: data.createdAt ? new Date(data.createdAt) : new Date(),
      updated_at: new Date()
    }
  }

  async transformUsageStats(key, data) {
    const parts = key.split(':')
    const date = parts[2]
    const keyId = parts[3]
    const model = parts[4]

    return {
      id: crypto.randomUUID(),
      date,
      api_key_id: keyId,
      model,
      request_count: parseInt(data.requestCount) || 0,
      token_count: parseInt(data.tokenCount) || 0,
      created_at: new Date(),
      updated_at: new Date()
    }
  }

  async validateTableMigration(tablePlan) {
    const { name, redisPattern, pgTable } = tablePlan

    logger.info(`ğŸ” éªŒè¯è¡¨è¿ç§»: ${name}`)

    try {
      // è·å–Redisè®°å½•æ•°é‡
      const redisKeys = await this.redisClient.keys(redisPattern)
      const redisCount = redisKeys.length

      // è·å–PostgreSQLè®°å½•æ•°é‡
      const pgResult = await this.pgPool.query(`SELECT COUNT(*) as count FROM ${pgTable}`)
      const pgCount = parseInt(pgResult.rows[0].count)

      logger.info(`ğŸ“Š ${name} éªŒè¯ç»“æœ: Redis=${redisCount}, PostgreSQL=${pgCount}`)

      if (redisCount !== pgCount) {
        logger.warn(`âš ï¸ ${name} è®°å½•æ•°é‡ä¸åŒ¹é…ï¼`)
      } else {
        logger.info(`âœ… ${name} éªŒè¯é€šè¿‡`)
      }
    } catch (error) {
      logger.error(`âŒ ${name} éªŒè¯å¤±è´¥: ${error.message}`)
    }
  }

  async generateMigrationReport() {
    const duration = Date.now() - this.stats.startTime
    const report = {
      summary: {
        duration: `${Math.round(duration / 1000)}ç§’`,
        totalRecords: this.stats.migrated + this.stats.failed + this.stats.skipped,
        migrated: this.stats.migrated,
        failed: this.stats.failed,
        skipped: this.stats.skipped,
        successRate: `${Math.round((this.stats.migrated / (this.stats.migrated + this.stats.failed)) * 100)}%`
      },
      tables: this.stats.tables,
      timestamp: new Date().toISOString()
    }

    // ä¿å­˜æŠ¥å‘Š
    const reportPath = path.join(__dirname, '../logs/migration-report.json')
    fs.writeFileSync(reportPath, JSON.stringify(report, null, 2))

    logger.info('ğŸ“‹ è¿ç§»æŠ¥å‘Šå·²ç”Ÿæˆ:')
    logger.info(`   æ€»è€—æ—¶: ${report.summary.duration}`)
    logger.info(`   æˆåŠŸè¿ç§»: ${report.summary.migrated}`)
    logger.info(`   å¤±è´¥è®°å½•: ${report.summary.failed}`)
    logger.info(`   è·³è¿‡è®°å½•: ${report.summary.skipped}`)
    logger.info(`   æˆåŠŸç‡: ${report.summary.successRate}`)
    logger.info(`   è¯¦ç»†æŠ¥å‘Š: ${reportPath}`)
  }

  async rollbackMigration() {
    logger.warn('ğŸ”„ å¼€å§‹è¿ç§»å›æ»š...')
    // å®ç°å›æ»šé€»è¾‘ï¼ˆå¯é€‰ï¼‰
    logger.info('âœ… å›æ»šå®Œæˆ')
  }

  async cleanup() {
    if (this.redisClient) {
      await this.redisClient.disconnect()
    }
    if (this.pgPool) {
      await this.pgPool.end()
    }
  }
}

// CLIå…¥å£
async function main() {
  const args = process.argv.slice(2)
  const options = {}

  // è§£æå‘½ä»¤è¡Œå‚æ•°
  for (let i = 0; i < args.length; i += 2) {
    const key = args[i].replace('--', '')
    const value = args[i + 1]

    if (value === 'true') {
      options[key] = true
    } else if (value === 'false') {
      options[key] = false
    } else if (!isNaN(value)) {
      options[key] = parseInt(value)
    } else {
      options[key] = value
    }
  }

  const migrationService = new DataMigrationService()

  try {
    await migrationService.initialize()
    await migrationService.migrateData(options)
  } catch (error) {
    logger.error(`è¿ç§»å¤±è´¥: ${error.message}`)
    process.exit(1)
  } finally {
    await migrationService.cleanup()
  }
}

// å¦‚æœç›´æ¥è¿è¡Œæ­¤è„šæœ¬
if (require.main === module) {
  main().catch(console.error)
}

module.exports = DataMigrationService
